{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predicting tweet sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from https://www.kaggle.com/datasets/bhavikjikadara/tweets-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context**\n",
    "\n",
    "This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the Twitter API. The tweets have been annotated (0 = negative, 4 = positive) and can be used to detect sentiment.\n",
    "\n",
    "**Content**\n",
    "\n",
    "It contains the following 6 fields:\n",
    "\n",
    "* target: the polarity of the tweet (0 = negative and 4 = positive)\n",
    "* ids: The id of the tweet ( 2087)\n",
    "* date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "* flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "* user: the user that tweeted.\n",
    "* text: the text of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Exploratory Data Analysis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# text processing libraries\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "from collections import Counter\n",
    "# import string\n",
    "import nltk\n",
    "# import warnings\n",
    "# %matplotlib inline\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas and Numpy have been used for data manipulation and numerical calculations\n",
    "\n",
    "Matplotlib and Seaborn have been used for data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data   \n",
    "tweets = pd.read_csv(\"..//data//tweets.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Splitting dataset into training, valid and testing parts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_valid, x_test, y_train_valid, y_test = train_test_split(\n",
    "    tweets, # X\n",
    "    tweets[tweets.columns.values[1:6]], # y\n",
    "    test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_valid.shape, y_train_valid.shape, x_test.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_valid, # X\n",
    "    y_train_valid, # y\n",
    "    test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.to_csv(\"..//data//x_train.csv\", index=False)\n",
    "#y_train.to_csv(\"..//data//y_train.csv\", index=False)\n",
    "#x_valid.to_csv(\"..//data//x_valid.csv\", index=False)\n",
    "#y_valid.to_csv(\"..//data//y_valid.csv\", index=False)\n",
    "#x_test.to_csv(\"..//data//x_test.csv\", index=False)\n",
    "#y_test.to_csv(\"..//data//y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the dataframe\n",
    "df = x_train\n",
    "print(\"Shape of the dataframe:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the last few rows of the dataframe\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display information about data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplication\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics of numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data reduction**\n",
    "\n",
    "Some columns or variables can be dropped if they do not add value to our analysis\n",
    "\n",
    "In our dataset, columns ID, Date, flag, User donâ€™t have any predictive power to predict the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(['ID', 'Date', 'flag', 'User'], axis = 'columns')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data cleaning**\n",
    "\n",
    "Some names of the variables are not relevant and not easy to understand\n",
    "\n",
    "Some data may have data entry errors, and some variables may need data type conversion. We need to fix this issue in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting target values\n",
    "data['Target'] = data['Target'].replace(4, 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary user tags\n",
    "data['Text'] = data['Text'].replace(r\"@\\w+\", \"\", regex=True)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolving contractions (and slang)\n",
    "data['Text'] = data['Text'].apply(lambda x: contractions.fix(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation marks\n",
    "data['Text'] = data['Text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercasing letters in the text\n",
    "data['Text'] = data['Text'].str.lower()\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize class distribution\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x = 'Target' , data = data)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore tweet length\n",
    "data['characters'] = data['Text'].apply(lambda x: len(x))\n",
    "\n",
    "# visualize tweet length distribution\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.histplot(data['characters'], bins = 60)\n",
    "plt.title('Distribution of tweet length')\n",
    "plt.xlabel('Number of characters')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore tweet length\n",
    "data['words'] = data['Text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# visualize tweet length distribution\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.histplot(data['words'], bins = 30)\n",
    "plt.title('Distribution of tweet length')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combine all the text into a single string\n",
    "all_text = ' '.join(data['Text'])\n",
    "\n",
    "# split the text into individual words\n",
    "words = all_text.split()\n",
    "\n",
    "# count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# get the top 10 most common words\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "# extract the words and their counts\n",
    "top_10_words, top_10_counts = zip(*top_10_words)\n",
    "\n",
    "# plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10_words, top_10_counts)\n",
    "plt.title('Top 10 Most Common Words')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word count depending on sentiment\n",
    "\n",
    "df = data.groupby(\"Target\").words.agg(\"mean\")\n",
    "\n",
    "df.plot(kind = 'bar', color = 'blue')\n",
    "\n",
    "plt.title('Average word count depending on sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Average word count')\n",
    "plt.xticks(rotation = 0)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is # in tweet?\n",
    "\n",
    "data['has_hashtag'] = tweets['Text'].str.contains(r'#\\w+')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is hashtag present in negatives tweets?\n",
    "\n",
    "data[data['Target'] == 0]['has_hashtag'].value_counts().apply(lambda x: x / len(data[data['Target'] == 0]) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is hashtag present in positives tweets?\n",
    "\n",
    "data[data['Target'] == 1]['has_hashtag'].value_counts().apply(lambda x: x / len(data[data['Target'] == 1]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is \"not\" in tweet?\n",
    "\n",
    "data['has_not'] = data['Text'].str.contains('not')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is \"not\" present in negatives tweets?\n",
    "\n",
    "data[data['Target'] == 0]['has_not'].value_counts().apply(lambda x: x / len(data[data['Target'] == 0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is \"not\" present in positives tweets?\n",
    "\n",
    "data[data['Target'] == 1]['has_not'].value_counts().apply(lambda x: x / len(data[data['Target'] == 1]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract hour from the Date column\n",
    "\n",
    "data['Hour'] = pd.to_datetime(tweets['Date']).dt.hour\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distribution of tweets over the day\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x = 'Hour', data = data, color = 'blue')\n",
    "plt.title('Distribution of tweets over the day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the the influence of the hour of writing a tweet on the Target variable\n",
    "hourly_target_counts = data.groupby('Hour')['Target'].value_counts().unstack(fill_value=0)\n",
    "plt.figure(figsize=(15, 6))\n",
    "hourly_target_counts.plot(kind='bar', stacked=True)\n",
    "\n",
    "plt.title('The influence of the hour of writing a tweet on the sentiment')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation = 0)  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting words which have less characters than 3\n",
    "\n",
    "data['clean_text'] = data[\"Text\"].apply(lambda x: \" \".join([w for w in x.split() if len(w)>=3]))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual words considered as tokens\n",
    "\n",
    "tokenized_tweet = data['clean_text'].apply(lambda x: x.split())\n",
    "tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#tokenized_tweet = tokenized_tweet.apply(lambda s: [stemmer.stem(word) for word in s]) # stemming\n",
    "#tokenized_tweet\n",
    "# Initialize wordnet lemmatizer only on verbs - makes the biggest sense\n",
    "wnl = WordNetLemmatizer()\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda s: [wnl.lemmatize(word, pos=\"v\") for word in s]) # lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet.iloc[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining to sentences\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "data['clean_text'] = tokenized_tweet\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in data['clean_text']])\n",
    "all_words_pos = ' '.join([text for text in data['clean_text'][data['Target'] == 1]])\n",
    "all_words_neg = ' '.join([text for text in data['clean_text'][data['Target'] == 0]])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n",
    "wordcloud_pos = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words_pos)\n",
    "wordcloud_neg = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words_neg)\n",
    "\n",
    "# plot the graph\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "ax[0].imshow(wordcloud, interpolation=\"bilinear\")\n",
    "ax[0].set_title('All words')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "ax[1].set_title('Words target 1 - Positive')\n",
    "ax[1].axis('off')\n",
    "ax[2].imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "ax[2].set_title('Words target 0 - Negative')\n",
    "ax[2].axis('off')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_extract(tweets):\n",
    "    hashtags = []\n",
    "    for tweet in tweets:\n",
    "        ht = re.findall(r\"#(\\w+)\", tweet)\n",
    "        hashtags.append(ht)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting hashtags from positive tweets\n",
    "ht_positive = hashtag_extract(tweets['Text'][data['Target'] == 1])\n",
    "\n",
    "# extracting hashtags from negative tweets\n",
    "ht_negative = hashtag_extract(tweets['Text'][data['Target'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnest list \n",
    "ht_positive = sum(ht_positive, [])\n",
    "ht_negative = sum(ht_negative, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_positive[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_negative[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dictionary to dataframe\n",
    "freq = nltk.FreqDist(ht_positive)\n",
    "d = pd.DataFrame({'Hashtag': list(freq.keys()), \n",
    "                  'Count': list(freq.values())\n",
    "                  })\n",
    "d.sort_values(by='Count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting top 10 most frequent hashtags positive\n",
    "d = d.nlargest(columns=\"Count\", n = 10)\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(data=d, x= \"Hashtag\", y = \"Count\", color=\"blue\")\n",
    "plt.title('Top 10 most frequent hashtags in positive tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dictionary to dataframe\n",
    "freq = nltk.FreqDist(ht_negative)\n",
    "d = pd.DataFrame({'Hashtag': list(freq.keys()), \n",
    "                  'Count': list(freq.values())\n",
    "                  })\n",
    "d.sort_values(by='Count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting top 10 most frequent hashtags negative\n",
    "d = d.nlargest(columns=\"Count\", n = 10)\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(data=d, x= \"Hashtag\", y = \"Count\", color=\"blue\")\n",
    "plt.title('Top 10 most frequent hashtags in negative tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Feature engineering** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = x_train.drop(['ID', 'Date', 'flag', 'User'], axis = 'columns')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Target'] = data['Target'].replace(4, 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary user tags\n",
    "data['Text'] = data['Text'].replace(r\"@\\w+\", \"\", regex=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolving contractions (and slang)\n",
    "data['Text'] = data['Text'].apply(lambda x: contractions.fix(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation marks\n",
    "data['Text'] = data['Text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting websites\n",
    "data['Text'] = data['Text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercasing letters in the text\n",
    "data['Text'] = data['Text'].str.lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting words which have less characters than 2\n",
    "\n",
    "data['Text'] = data[\"Text\"].apply(lambda x: \" \".join([w for w in x.split() if len(w)>=2]))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual words considered as tokens\n",
    "\n",
    "tokenized_tweet = data['Text'].apply(lambda x: x.split())\n",
    "tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wordnet lemmatizer only on verbs - makes the biggest sense\n",
    "wnl = WordNetLemmatizer()\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda s: [wnl.lemmatize(word, pos=\"v\") for word in s]) # lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining to sentences\n",
    "combined_sentences = [' '.join(tokens) for tokens in tokenized_tweet]\n",
    "data['combined_tweet'] = combined_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df = 0.90, min_df = 2, max_features = 1000, stop_words='english')\n",
    "bow = bow_vectorizer.fit_transform(data['combined_tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Modeling and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
